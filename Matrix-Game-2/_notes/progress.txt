

* added profiling and bencnmarkign script inside inference.py

benchmarking - more runs, get the metrics
```
python inference.py   --config_path configs/inference_yaml/inference_universal.yaml   --img_path demo_images/universal/0001.png   --warmup_blocks 1 --max_blocks 4   --profile_focus all --profile_timesteps 2   --skip_video_write
```


Profiling all inference pipeline - less runs, get the bottlenecks
```
python inference.py     --config_path configs/inference_yaml/inference_universal.yaml     -img_path demo_images/universal/0000.png      --num_output_frames 150     --output_folder outputs/     --pretrained_model_path /home/mac_local/Matrix-Game/Matrix-Game-2/Matrix-Game-2.0 --profile --torch_profile   --warmup_blocks 2   --max_blocks 6   --torch_profile   --torch_profile_dir prof_runs/matrix2_small   --skip_video_write
```
which generated the profiler_0000.png image. 
Here note that VAE decode dominates, but is polluted by torch compile calls, so not a great representation. 
The denoise and context update is dominated by convolutions - aten::conv3d, aten::convolution, aten::conv_dilated_3d. So they are good targets to optimize

---

To remove the torch compile work, I added three changes - in-process prewarm up step, resetting peak VRAM, start torch.profiler and run the measured blocks to remove no compile. However this didnt work out and produced very similar profiling result to profiler_0000.png. 
After the code edit, the following code were run, first for warm up and second for profiling
```
python inference.py \
  --config_path configs/inference_yaml/inference_universal.yaml \
  --img_path demo_images/universal/0000.png \
  --num_output_frames 150 \
  --output_folder outputs/ \
  --pretrained_model_path /home/mac_local/Matrix-Game/Matrix-Game-2/Matrix-Game-2.0 \
  --bench \
  --warmup_blocks 4 \
  --max_blocks 8

```

then
```
python inference.py \
  --config_path configs/inference_yaml/inference_universal.yaml \
  --img_path demo_images/universal/0000.png \
  --num_output_frames 150 \
  --output_folder outputs/ \
  --pretrained_model_path /home/mac_local/Matrix-Game/Matrix-Game-2/Matrix-Game-2.0 \
  --bench --torch_profile \
  --warmup_blocks 2 \
  --max_blocks 4 \
  --torch_profile_dir prof_runs/matrix2_clean


```

From the profling results, it looks like the bottlenecks are VAE decode, overhead from torch compile, and plaves with lots of memory or overhead heavy operations like aten::fill_ -> 123k calls.
To be confident about memory vs compute bound we also need nsight systems, so we will do this after trying another crack at geting rid of torch compile overhead. 

---

Updated the inference.py code, to exclude the inital runs containing torch.compile. In pareto, when searching `compile.compile_inner`, it no longer shows up. The profiler image is in profiler_0002.png
Therefore, we have accomplished 
1. Getting rid of torch compile overhead
2. profling only the steady states of the world model's inference, broken up into denoise, context update, and VAE decode. 

```
python inference.py   --config_path configs/inference_yaml/inference_universal.yaml   --img_path demo_images/universal/0000.png   --num_output_frames 150   --output_folder outputs/   --pretrained_model_path /home/mac_local/Matrix-Game/Matrix-Game-2/Matrix-Game-2.0   --bench --torch_profile   --warmup_blocks 2   --max_blocks 4   --torch_profile_dir prof_runs/matrix2_clean
```

The dominating phase based on this profiling is denoise, then VAE decode, then context update.

We also obtain the cuda runtime for all phases, where the head of the results are
```
-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls  
-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
                                                denoise         0.00%       0.000us         0.00%       0.000us       0.000us        1.988s        92.02%        1.988s     496.952ms           0 B           0 B           0 B           0 B             4  
                                             vae_decode         0.00%       0.000us         0.00%       0.000us       0.000us        1.178s        54.54%        1.178s     294.531ms           0 B           0 B           0 B           0 B             4  
                              aten::slow_conv_dilated3d         2.21%      84.019ms        26.02%     990.846ms       2.815ms     771.123ms        35.70%     975.228ms       2.771ms           0 B           0 B      19.30 GB    -513.99 GB           352  
                                             ctx_update         0.00%       0.000us         0.00%       0.000us       0.000us     637.559ms        29.51%     637.559ms     159.390ms           0 B           0 B           0 B           0 B             4  
void at::native::vol2col_kernel<c10::Half>(long, c10...         0.00%       0.000us         0.00%       0.000us       0.000us     560.170ms        25.93%     560.170ms       1.667ms           0 B           0 B           0 B           0 B           336  
                                            aten::copy_         5.02%     191.240ms        21.17%     806.253ms      13.327us     306.391ms        14.18%     311.866ms       5.155us       1.03 KB     -15.22 KB           0 B           0 B         60500  
                                            aten::addmm         3.28%     124.910ms         4.13%     157.074ms      32.082us     155.321ms         7.19%     155.321ms      31.724us           0 B           0 B      49.07 GB      49.07 GB          4896  
                        flash_attn::_flash_attn_forward         0.83%      31.707ms         1.16%      44.167ms      92.014us     149.002ms         6.90%     149.308ms     311.058us           0 B           0 B       2.49 GB           0 B           480  
void flash::flash_fwd_kernel<Flash_fwd_kernel_traits...         0.00%       0.000us         0.00%       0.000us       0.000us     149.002ms         6.90%     149.002ms     310.422us           0 B           0 B           0 B           0 B           480  
void at::native::elementwise_kernel<128, 4, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us     140.404ms         6.50%     140.404ms       6.561us           0 B           0 B           0 B           0 B         21400  
                 flash_attn::_flash_attn_varlen_forward         1.73%      65.987ms         2.35%      89.576ms      93.308us     126.191ms         5.84%     126.673ms     131.951us           0 B           0 B       7.36 GB           0 B           960  
void flash::flash_fwd_kernel<Flash_fwd_kernel_traits...         0.00%       0.000us         0.00%       0.000us       0.000us     126.191ms         5.84%     126.191ms     131.449us           0 B           0 B           0 B           0 B           960  
                                              aten::mul         1.94%      73.963ms         3.45%     131.394ms       9.393us     122.592ms         5.68%     124.200ms       8.879us      39.75 KB      39.75 KB     109.97 GB     109.97 GB         13988  

```

denoise has Self CUDA = 1.988s and Self CPU = 0.000us
vae_decode has Self CUDA = 1.178s and Self CPU = 0.000us
ctx_update has Self CUDA = 637.6ms and Self CPU = 0.000us

Therefore GPU kernel execution is dominant. Looking at the operations, the ones taking up the most time is 
aten::slow_conv_dilated3d (CUDA total ~975ms)
vol2col_kernel (CUDA total ~560ms)
aten::addmm (CUDA total ~155ms)
flash_attn::_flash_attn_forward (CUDA total ~149ms)

If python CPU was the bottleneck, we will see more time here. 

Therefore the limiter is the GPU side compute/memory work from denoise and VAE decode.

---
Next we obtain the profiler results for denoise and VAE decode.

To obtain the cuda runtime for denoise, we also run the profiler focusing just on the denoise

```
python inference.py \
  --config_path configs/inference_yaml/inference_universal.yaml \
  --img_path demo_images/universal/0000.png \
  --num_output_frames 150 \
  --output_folder outputs/ \
  --pretrained_model_path /home/mac_local/Matrix-Game/Matrix-Game-2/Matrix-Game-2.0 \
  --bench --torch_profile \
  --warmup_blocks 2 \
  --max_blocks 6 \
  --profile_focus denoise \
  --profile_timesteps 0 \
  --torch_profile_dir prof_runs/matrix2_denoise_clean

```

where the head of the results are
```
-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls  
-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
                                                denoise         0.00%       0.000us         0.00%       0.000us       0.000us        2.997s       261.11%        2.997s     499.447ms           0 B           0 B           0 B           0 B             6  
                                            aten::copy_         6.71%     205.826ms        12.91%     395.817ms       8.893us     226.122ms        19.70%     226.151ms       5.081us       1.00 KB     -17.00 KB           0 B           0 B         44509  
                                            aten::addmm         5.35%     164.031ms         6.53%     200.159ms      35.948us     174.717ms        15.22%     174.904ms      31.412us           0 B           0 B      55.54 GB      55.54 GB          5568  
                        flash_attn::_flash_attn_forward         1.20%      36.783ms         1.71%      52.340ms      96.926us     166.994ms        14.55%     168.215ms     311.509us           0 B           0 B       2.80 GB           0 B           540  
void flash::flash_fwd_kernel<Flash_fwd_kernel_traits...         0.00%       0.000us         0.00%       0.000us       0.000us     166.994ms        14.55%     166.994ms     309.249us           0 B           0 B           0 B           0 B           540  
                 flash_attn::_flash_attn_varlen_forward         2.50%      76.590ms         3.40%     104.196ms      96.477us     131.425ms        11.45%     131.829ms     122.064us           0 B           0 B       8.29 GB           0 B          1080  
void flash::flash_fwd_kernel<Flash_fwd_kernel_traits...         0.00%       0.000us         0.00%       0.000us       0.000us     131.425ms        11.45%     131.425ms     121.690us           0 B           0 B           0 B           0 B          1080  
void at::native::elementwise_kernel<128, 4, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us     126.537ms        11.03%     126.537ms       4.056us           0 B           0 B           0 B           0 B         31194  
                                              aten::mul         2.64%      81.048ms         4.07%     124.718ms       7.989us     118.995ms        10.37%     119.038ms       7.625us      44.72 KB      44.72 KB     106.55 GB     106.55 GB         15612  
                                              aten::add         2.07%      63.363ms         3.37%     103.205ms      11.814us     111.999ms         9.76%     112.064ms      12.828us           0 B           0 B      33.02 GB      33.01 GB          8736  

```
and the profiler screenshot is in profiler_0004.png

For decode 

```
python inference.py \
  --config_path configs/inference_yaml/inference_universal.yaml \
  --img_path demo_images/universal/0000.png \
  --num_output_frames 150 \
  --output_folder outputs/ \
  --pretrained_model_path /home/mac_local/Matrix-Game/Matrix-Game-2/Matrix-Game-2.0 \
  --bench --torch_profile \
  --warmup_blocks 2 \
  --max_blocks 6 \
  --profile_focus decode \
  --profile_timesteps 0 \
  --torch_profile_dir prof_runs/matrix2_decode_clean

```

```

------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls  
-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
                                                denoise         0.00%       0.000us         0.00%       0.000us       0.000us        2.961s       103.63%        2.961s     493.445ms           0 B           0 B           0 B           0 B             6  
                                             vae_decode         0.00%       0.000us         0.00%       0.000us       0.000us        1.773s        62.08%        1.773s     295.575ms           0 B           0 B           0 B           0 B             6  
                              aten::slow_conv_dilated3d         2.27%     107.787ms        28.80%        1.365s       2.615ms        1.156s        40.48%        1.425s       2.730ms           0 B           0 B      28.92 GB    -770.99 GB           522  
void at::native::vol2col_kernel<c10::Half>(long, c10...         0.00%       0.000us         0.00%       0.000us       0.000us     840.172ms        29.41%     840.172ms       1.667ms           0 B           0 B           0 B           0 B           504  
                                            aten::copy_         5.09%     241.369ms        23.36%        1.107s      14.502us     385.904ms        13.51%     397.265ms       5.204us          32 B     -18.34 KB           0 B           0 B         76332  
void at::native::elementwise_kernel<128, 4, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us     210.601ms         7.37%     210.601ms       6.561us           0 B           0 B           0 B           0 B         32100  
             nvjet_hsh_384x96_64x3_2x1_v_badd_coopA_NNT         0.00%       0.000us         0.00%       0.000us       0.000us     181.738ms         6.36%     181.738ms       1.683ms           0 B           0 B           0 B           0 B           108  
                                            aten::addmm         2.99%     141.606ms         3.77%     178.456ms      32.399us     174.769ms         6.12%     174.769ms      31.730us           0 B           0 B      55.18 GB      55.18 GB          5508  
                        flash_attn::_flash_attn_forward         0.77%      36.290ms         1.06%      50.031ms      92.651us     167.632ms         5.87%     167.632ms     310.429us           0 B           0 B       2.80 GB           0 B           540  
void flash::flash_fwd_kernel<Flash_fwd_kernel_traits...         0.00%       0.000us         0.00%       0.000us       0.000us     167.632ms         5.87%     167.632ms     310.429us           0 B           0 B           0 B           0 B           540  
                                              aten::mul         1.76%      83.415ms         3.24%     153.340ms       9.699us     144.274ms         5.05%     146.700ms       9.279us      44.72 KB      44.72 KB     129.51 GB     129.51 GB         15810  
                                    Command Buffer Full        14.67%     695.291ms        14.67%     695.291ms     299.824us     143.620ms         5.03%     143.620ms      61.932us           0 B           0 B           0 B           0 B          2319  
                                              aten::cat         1.65%      78.217ms         3.31%     157.029ms      19.272us     142.206ms         4.98%     147.072ms      18.050us           0 B           0 B     118.39 GB     118.39 GB          8148  
                 flash_attn::_flash_attn_varlen_forward         1.59%      75.300ms         2.16%     102.207ms      94.636us     142.018ms         4.97%     142.259ms     131.721us           0 B           0 B       8.29 GB           0 B          1080  
void flash::flash_fwd_kernel<Flash_fwd_kernel_traits...         0.00%       0.000us         0.00%       0.000us       0.000us     142.018ms         4.97%     142.018ms     131.498us           0 B           0 B           0 B           0 B          1080  
                                              aten::add         1.39%      66.055ms         3.18%     150.646ms      16.839us     136.146ms         4.77%     139.000ms      15.538us           0 B           0 B      57.63 GB      57.63 GB          8946  
void at::native::elementwise_kernel<128, 4, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us     128.061ms         4.48%     128.061ms       4.090us           0 B           0 B           0 B           0 B         31314  
         nvjet_tst_128x240_64x4_2x1_v_bz_coopA_bias_TNT         0.00%       0.000us         0.00%       0.000us       0.000us     107.284ms         3.76%     107.284ms      28.382us           0 B           0 B           0 B           0 B          3780  
void at::native::elementwise_kernel<128, 4, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us      95.224ms         3.33%      95.224ms      24.954us           0 B           0 B           0 B           0 B          3816  
            nvjet_hsh_192x192_64x3_2x1_v_badd_coopB_NNN         0.00%       0.000us         0.00%       0.000us       0.000us      94.158ms         3.30%      94.158ms     871.835us           0 B           0 B           0 B           0 B           108  
## Call CompiledFxGraph falkigfpqu63d44rrli4yuhn3ozj...         0.00%       0.000us         0.00%       0.000us       0.000us      64.509ms         2.26%      64.509ms       3.584ms           0 B           0 B           0 B           0 B            18  
                         Memcpy DtoD (Device -> Device)         0.00%       0.000us         0.00%       0.000us       0.000us      61.191ms         2.14%      61.191ms      13.193us           0 B           0 B           0 B           0 B          4638  

```
and the profiler result is in profiler_0003.png. Note that we need the latent variables from denoise, so this is included in the profiler. 

From the above tables and profiler results we see that
- Denoise is the biggest wall-time bucket 
    2.961s total (avg ~493ms per block call)
- Decode is next
    1.773s total (avg ~296ms per block call) which is ~38% of (denoise + decode) time
- VAE decode 
    aten::slow_conv_dilated3d ~1.156s in self CUDA, 522 calls
    vol2col_kernel ~840ms self CUDA, 504 calls
    The pair tells 3D conv lowering/im2col fallback
- “Command Buffer Full”
    ~695ms CPU self in Command Buffer Full plus ~143ms CUDA, which means too many small kernels / launch pressure, GPU command buffer saturating causing CPU stalls.


---

optimizing the vae_decoder

```
python inference.py --config_path configs/inference_yaml/inference_universal.yaml --img_path demo_images/universal/0000.png --num_output_frames 150 --output_folder outputs/ --pretrained_model_path /home/mac_local/Matrix-Game/Matrix-Game-2/Matrix-Game-2.0 --bench --torch_profile --warmup_blocks 2 --max_blocks 6 --profile_focus decode --profile_timesteps 0 --torch_profile_dir prof_runs/matrix2_decode_clean_optimized
```

The bottleneck is in 

-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls  
-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
                                             vae_decode         0.00%       0.000us         0.00%       0.000us       0.000us        1.768s       104.22%        1.768s     294.644ms           0 B           0 B           0 B           0 B             6  
                              aten::slow_conv_dilated3d         3.63%      64.294ms        56.03%     992.500ms       1.901ms        1.157s        68.18%        1.393s       2.668ms           0 B           0 B      28.82 GB    -771.04 GB           522  
void at::native::vol2col_kernel<c10::Half>(long, c10...         0.00%       0.000us         0.00%       0.000us       0.000us     840.309ms        49.54%     840.309ms       1.610ms           0 B           0 B           0 B           0 B           522  
void at::native::elementwise_kernel<128, 4, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us     210.972ms        12.44%     210.972ms       6.571us           0 B           0 B           0 B           0 B         32106  
             nvjet_hsh_384x96_64x3_2x1_v_badd_coopA_NNT         0.00%       0.000us         0.00%       0.000us       0.000us     181.142ms        10.68%     181.142ms       1.677ms           0 B           0 B           0 B           0 B           108  
                                    Command Buffer Full        41.05%     727.232ms        41.05%     727.232ms     303.013us     175.302ms        10.33%     175.302ms      73.042us           0 B           0 B           0 B           0 B          2400  
                                            aten::copy_         5.01%      88.760ms        45.17%     800.156ms      25.296us     157.095ms         9.26%     167.810ms       5.305us           0 B        -384 B           0 B           0 B         31632  
            nvjet_hsh_192x192_64x3_2x1_v_badd_coopB_NNN         0.00%       0.000us         0.00%       0.000us       0.000us      94.256ms         5.56%      94.256ms     872.738us           0 B           0 B           0 B           0 B           108  
## Call CompiledFxGraph falkigfpqu63d44rrli4yuhn3ozj...         0.00%       0.000us         0.00%       0.000us       0.000us      64.576ms         3.81%      64.576ms       3.588ms           0 B           0 B           0 B           0 B            18  
## Call CompiledFxGraph ftoojlkuckf4mrwgvx3llv3dyxvr...         0.00%       0.000us         0.00%       0.000us       0.000us      60.390ms         3.56%      60.390ms       3.355ms           0 B           0 B           0 B           0 B            18  
                                              aten::cat         0.33%       5.792ms         2.57%      45.479ms     161.273us      55.693ms         3.28%      61.737ms     218.925us           0 B           0 B      40.13 GB      40.13 GB           282  

aten::slow_conv_dilated3d
from the profiler it is diagnosed that this comes from CausalConv3d, and the forward pass is not optimzied. 
Here we update the looping logic where it is processing one frame units at a time to doing all in one go. 

The result is 

-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls  
-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  
                                             vae_decode         0.00%       0.000us         0.00%       0.000us       0.000us     701.085ms       107.77%     701.085ms     116.848ms           0 B           0 B           0 B           0 B             6  
                                aten::cudnn_convolution         2.17%      15.319ms         3.99%      28.170ms      62.600us     277.045ms        42.59%     278.595ms     619.099us           0 B           0 B      34.06 GB      33.56 GB           450  
sm90_xmma_fprop_implicit_gemm_f16f16_f16f32_f32_nhwc...         0.00%       0.000us         0.00%       0.000us       0.000us     126.706ms        19.48%     126.706ms       1.173ms           0 B           0 B           0 B           0 B           108  
sm90_xmma_fprop_implicit_gemm_f16f16_f16f32_f32_nhwc...         0.00%       0.000us         0.00%       0.000us       0.000us     111.396ms        17.12%     111.396ms     515.721us           0 B           0 B           0 B           0 B           216  
void at::native::elementwise_kernel<128, 4, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us      88.388ms        13.59%      88.388ms     118.801us           0 B           0 B           0 B           0 B           744  
                                            aten::copy_         0.60%       4.260ms         1.61%      11.337ms      11.810us      86.295ms        13.26%      86.304ms      89.900us          32 B        -352 B           0 B           0 B           960  
void at::native::elementwise_kernel<128, 4, at::nati...         0.00%       0.000us         0.00%       0.000us       0.000us      51.265ms         7.88%      51.265ms     155.347us           0 B           0 B           0 B           0 B           330  
                                             aten::add_         0.30%       2.130ms         0.51%       3.597ms      11.754us      45.644ms         7.02%      45.644ms     149.164us           0 B           0 B           0 B           0 B           306  
                                              aten::cat         0.92%       6.475ms         1.86%      13.127ms      41.280us      41.264ms         6.34%      41.264ms     129.760us           0 B           0 B      41.22 GB      41.22 GB           318  
                         Memcpy DtoD (Device -> Device)         0.00%       0.000us         0.00%       0.000us       0.000us      38.968ms         5.99%      38.968ms      48.832us           0 B           0 B           0 B           0 B           798  
                              aten::slow_conv_dilated3d         0.85%       5.987ms         1.87%      13.221ms      73.453us      26.043ms         4.00%      26.432ms     146.846us           0 B           0 B     529.40 MB     -12.24 GB           180  


So vae_decode average cuda time went down from 294.644ms to 116.848ms , and the bottleneck,
aten::slow_conv_dilated3d's calls changed from  2.668ms CUDA time average, 522 calls to  146.846us, and  180 respectably. 

We also profile the end to end runs including denoise, decode and context update and we see this optimimzation scaling when profiling the whole. 
Comparing the decode profilers first, we see that the runs for each block for the baseline is ~295ms and the optimized is ~116ms, which matches what we got in the table. 
In the end to end, we can see that the "decode" section, highlighted in caramel color is much much shorter. 

Here because we see consistencies between the whole and the decode where performance gain is clear, we move forward. 

---

In the previous run, i commented out # current_vae_decoder.to(memory_format=torch.channels_last_3d) which caused the block ms time to be worse. Therefore fixed it that supports 5D